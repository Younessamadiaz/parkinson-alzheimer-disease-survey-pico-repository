# -*- coding: utf-8 -*-
"""Untitled36.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QdLsWxzC19i-_gPc7fsa7nDSUOuQpM5u
"""

pip install metapub

# ---- query_pmc_with_metapub.py ----
from time import sleep
from pathlib import Path
import json

# Core MetaPub pieces (typical usage)
from metapub import PubMedFetcher
# Some versions expose convert helpers like this:
try:
    from metapub.convert import pmid2pmcid
except Exception:
    pmid2pmcid = None  # fallback handled below

OUT_DIR = Path("out_pmc")
OUT_DIR.mkdir(exist_ok=True)

def build_query():
    # EXAMPLE: change this to your topic/filters
    # English: papers about "microbiome" in 2024–2025, human studies, open to tweak.
    # Français : articles sur “microbiome” en 2024–2025, études humaines.
    terms = [
        'microbiome',
        'humans[MeSH Terms]',
        '2024:3000[dp]'  # date range; adjust as needed
    ]
    return " AND ".join(terms)

def main():
    query = build_query()
    fetch = PubMedFetcher()

    # 1) Find PMIDs for the query (set a sensible retmax)
    pmids = fetch.pmids_for_query(query, retmax=200)  # increase if needed

    results = []
    for i, pmid in enumerate(pmids, 1):
        # Be gentle with NCBI (rate limiting)
        if i % 10 == 0:
            sleep(0.4)

        # 2) Fetch article metadata
        art = fetch.article_by_pmid(pmid)

        # Safe attribute access (fields vary by record)
        meta = {
            "pmid": pmid,
            "title": getattr(art, "title", None),
            "journal": getattr(art, "journal", None),
            "year": getattr(art, "year", None),
            "doi": getattr(art, "doi", None),
            "authors": getattr(art, "authors", None),
        }

        # 3) Try to map PMID -> PMCID to know if PMC full text exists
        pmcid = None
        if pmid2pmcid:
            try:
                pmcid = pmid2pmcid(pmid)
            except Exception:
                pmcid = None
        meta["pmcid"] = pmcid

        # 4) (Optional) If you need full text, you can fetch from PMC when pmcid exists.
        # Depending on MetaPub version, there may be helpers; otherwise you can
        # resolve a PMC OA URL or use the OAI-PMH endpoint. Kept minimal here.
        # meta["fulltext_xml"] = get_pmc_fulltext_xml(pmcid)  # implement if needed

        results.append(meta)

    # 5) Save results (JSON lines for easy re-use)
    with open(OUT_DIR / "pubmed_results.jsonl", "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    print(f"Saved {len(results)} records to {OUT_DIR / 'pubmed_results.jsonl'}")

if __name__ == "__main__":
    main()

pip install metapub pandas tenacity

# Commented out IPython magic to ensure Python compatibility.
# %%writefile finder_pubmed_keywords.py
# # -*- coding: utf-8 -*-
# from __future__ import annotations
# import argparse, csv, json, os, sys
# from pathlib import Path
# from time import sleep
# from datetime import datetime
# from typing import List, Dict, Any, Optional
# 
# import pandas as pd
# from tenacity import retry, stop_after_attempt, wait_exponential
# from metapub import PubMedFetcher
# try:
#     from metapub.convert import pmid2pmcid
# except Exception:
#     pmid2pmcid = None
# 
# def build_default_queries() -> List[str]:
#     q1 = ('("Parkinson Disease"[MeSH Terms] OR parkinson*[Title/Abstract]) '
#           'AND (technology OR technologies OR wearable* OR sensor* OR '
#           '"digital biomarker*" OR smartphone* OR "mobile app*" OR '
#           '"deep brain stimulation" OR DBS OR "machine learning" OR '
#           '"artificial intelligence" OR AI)')
#     q2 = ('("Alzheimer Disease"[MeSH Terms] OR alzheimer*[Title/Abstract] OR dementia[Title/Abstract]) '
#           'AND (technology OR technologies OR wearable* OR sensor* OR '
#           '"assistive technolog*" OR "smart home" OR "digital biomarker*" OR '
#           '"Internet of Things" OR IoT OR "machine learning" OR '
#           '"artificial intelligence" OR AI OR eye-tracking OR "speech analysis")')
#     return [q1, q2]
# 
# def apply_filters(q: str, since: Optional[int], until: Optional[int], humans: bool, pmc_oa: bool) -> str:
#     parts = [q]
#     if since or until:
#         s = since if since else 1800
#         u = until if until else 3000
#         parts.append(f"({s}:{u}[dp])")
#     if humans:
#         parts.append("humans[MeSH Terms]")
#     if pmc_oa:
#         parts.append("pmc open access[filter]")
#     return " AND ".join(parts)
# 
# @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=0.8, min=1, max=8))
# def fetch_article(fetcher: PubMedFetcher, pmid: str):
#     return fetcher.article_by_pmid(pmid)
# 
# def one_meta(art) -> Dict[str, Any]:
#     def g(name, default=None): return getattr(art, name, default)
#     try:
#         authors = "; ".join(f"{a.lastname} {a.initials}" for a in (art.authors or []))
#     except Exception:
#         authors = None
#     return {
#         "pmid": g("pmid"), "pmcid": None, "title": g("title"), "journal": g("journal"),
#         "year": g("year"), "volume": g("volume"), "issue": g("issue"), "pages": g("pages"),
#         "doi": g("doi"), "authors": authors, "first_author": g("first_author"),
#         "last_author": g("last_author"), "epubdate": g("epubdate"), "pubdate": g("pubdate"),
#         "mesh": "; ".join(g("meshheadings", []) or []), "affiliations": "; ".join(g("affiliations", []) or []),
#         "url": g("url"),
#     }
# 
# def map_pmid_to_pmcid_safe(pmid: str) -> Optional[str]:
#     if pmid2pmcid is None: return None
#     try: return pmid2pmcid(pmid)
#     except Exception: return None
# 
# def search_and_collect(query: str, retmax: int, sleep_every: int = 10, sleep_s: float = 0.35) -> List[Dict[str, Any]]:
#     fetch = PubMedFetcher()
#     pmids = fetch.pmids_for_query(query, retmax=retmax)
#     results: List[Dict[str, Any]] = []
#     for i, pmid in enumerate(pmids, 1):
#         if i % sleep_every == 0: sleep(sleep_s)
#         art = fetch_article(fetch, pmid)
#         meta = one_meta(art)
#         meta["pmcid"] = map_pmid_to_pmcid_safe(pmid)
#         results.append(meta)
#     return results
# 
# def save_results(rows: List[Dict[str, Any]], out_prefix: Path):
#     out_prefix.parent.mkdir(parents=True, exist_ok=True)
#     jsonl_path = out_prefix.with_suffix(".jsonl")
#     with open(jsonl_path, "w", encoding="utf-8") as f:
#         for r in rows: f.write(json.dumps(r, ensure_ascii=False) + "\n")
#     csv_path = out_prefix.with_suffix(".csv")
#     pd.DataFrame(rows).to_csv(csv_path, index=False, quoting=csv.QUOTE_MINIMAL)
#     return jsonl_path, csv_path
# 
# def parse_args_notebook_safe(parser: argparse.ArgumentParser):
#     if any(m in sys.modules for m in ("ipykernel", "google.colab")):
#         args, _ = parser.parse_known_args(); return args
#     return parser.parse_args()
# 
# def main():
#     parser = argparse.ArgumentParser(description="Trouver des références PubMed pour des mots-clés donnés.")
#     parser.add_argument("--query", action="append", help="Requête PubMed (peut être répétée).")
#     parser.add_argument("--since", type=int, default=2024, help="Année min (dp).")
#     parser.add_argument("--until", type=int, default=2025, help="Année max (dp).")
#     parser.add_argument("--humans", action="store_true", help="Filtre humans[MeSH Terms]")
#     parser.add_argument("--pmc_oa", action="store_true", help="PMC Open Access uniquement")
#     parser.add_argument("--retmax", type=int, default=300, help="Nb max d'articles par requête")
#     parser.add_argument("--outdir", type=Path, default=Path("results_pubmed"), help="Dossier de sortie")
#     args = parse_args_notebook_safe(parser)
# 
#     if not os.environ.get("NCBI_API_KEY"):
#         print("⚠️  NCBI_API_KEY non défini — optionnel mais recommandé.")
# 
#     queries = args.query if args.query else build_default_queries()
#     timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
#     all_rows: List[Dict[str, Any]] = []
# 
#     for idx, base_q in enumerate(queries, 1):
#         q = apply_filters(base_q, since=args.since, until=args.until, humans=args.humans, pmc_oa=args.pmc_oa)
#         print(f"[{idx}/{len(queries)}] Query:\n{q}\n")
#         rows = search_and_collect(q, retmax=args.retmax)
#         print(f"  -> {len(rows)} résultats")
#         all_rows.extend([dict(r, query=q) for r in rows])
#         prefix = args.outdir / f"q{idx}_{timestamp}"
#         jsonl_path, csv_path = save_results(rows, prefix)
#         print(f"  Saved: {jsonl_path} ; {csv_path}")
# 
#     if all_rows:
#         merged_prefix = args.outdir / f"merged_{timestamp}"
#         jsonl_path, csv_path = save_results(all_rows, merged_prefix)
#         print(f"\nMerged files:\n  {jsonl_path}\n  {csv_path}")
#     else:
#         print("Aucun résultat — assouplis la requête ou augmente --retmax.")
# 
# def run_finder(queries: Optional[List[str]] = None, since: int = 2024, until: int = 2025,
#                humans: bool = True, pmc_oa: bool = False, retmax: int = 300, outdir: str = "results_pubmed"):
#     if not queries: queries = build_default_queries()
#     if not os.environ.get("NCBI_API_KEY"):
#         print("⚠️  NCBI_API_KEY non défini — optionnel mais recommandé.")
#     qlist = [apply_filters(q, since=since, until=until, humans=humans, pmc_oa=pmc_oa) for q in queries]
#     timestamp = datetime.now().strftime("%Y%m%d-%H%M%S"); all_rows: List[Dict[str, Any]] = []
#     for idx, q in enumerate(qlist, 1):
#         print(f"[{idx}/{len(qlist)}] Query:\n{q}\n")
#         rows = search_and_collect(q, retmax=retmax)
#         print(f"  -> {len(rows)} résultats")
#         all_rows.extend([dict(r, query=q) for r in rows])
#         prefix = Path(outdir) / f"q{idx}_{timestamp}"
#         jsonl_path, csv_path = save_results(rows, prefix)
#         print(f"  Saved: {jsonl_path} ; {csv_path}")
#     if all_rows:
#         merged_prefix = Path(outdir) / f"merged_{timestamp}"
#         jsonl_path, csv_path = save_results(all_rows, merged_prefix)
#         print(f"\nMerged files:\n  {jsonl_path}\n  {csv_path}")
#     else:
#         print("Aucun résultat.")
# 
# if __name__ == "__main__":
#     main()
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile finder_pubmed_keywords.py
# # -*- coding: utf-8 -*-
# from __future__ import annotations
# import argparse, csv, json, os, sys
# from pathlib import Path
# from time import sleep
# from datetime import datetime
# from typing import List, Dict, Any, Optional
# 
# import pandas as pd
# from tenacity import retry, stop_after_attempt, wait_exponential
# from metapub import PubMedFetcher
# try:
#     from metapub.convert import pmid2pmcid
# except Exception:
#     pmid2pmcid = None
# 
# def build_default_queries() -> List[str]:
#     q1 = ('("Parkinson Disease"[MeSH Terms] OR parkinson*[Title/Abstract]) '
#           'AND (technology OR technologies OR wearable* OR sensor* OR '
#           '"digital biomarker*" OR smartphone* OR "mobile app*" OR '
#           '"deep brain stimulation" OR DBS OR "machine learning" OR '
#           '"artificial intelligence" OR AI)')
#     q2 = ('("Alzheimer Disease"[MeSH Terms] OR alzheimer*[Title/Abstract] OR dementia[Title/Abstract]) '
#           'AND (technology OR technologies OR wearable* OR sensor* OR '
#           '"assistive technolog*" OR "smart home" OR "digital biomarker*" OR '
#           '"Internet of Things" OR IoT OR "machine learning" OR '
#           '"artificial intelligence" OR AI OR eye-tracking OR "speech analysis")')
#     return [q1, q2]
# 
# def apply_filters(q: str, since: Optional[int], until: Optional[int], humans: bool, pmc_oa: bool) -> str:
#     parts = [q]
#     if since or until:
#         s = since if since else 1800
#         u = until if until else 3000
#         parts.append(f"({s}:{u}[dp])")
#     if humans:
#         parts.append("humans[MeSH Terms]")
#     if pmc_oa:
#         parts.append("pmc open access[filter]")
#     return " AND ".join(parts)
# 
# @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=0.8, min=1, max=8))
# def fetch_article(fetcher: PubMedFetcher, pmid: str):
#     return fetcher.article_by_pmid(pmid)
# 
# def one_meta(art) -> Dict[str, Any]:
#     def g(name, default=None): return getattr(art, name, default)
#     try:
#         authors = "; ".join(f"{a.lastname} {a.initials}" for a in (art.authors or []))
#     except Exception:
#         authors = None
#     return {
#         "pmid": g("pmid"), "pmcid": None, "title": g("title"), "journal": g("journal"),
#         "year": g("year"), "volume": g("volume"), "issue": g("issue"), "pages": g("pages"),
#         "doi": g("doi"), "authors": authors, "first_author": g("first_author"),
#         "last_author": g("last_author"), "epubdate": g("epubdate"), "pubdate": g("pubdate"),
#         "mesh": "; ".join(g("meshheadings", []) or []), "affiliations": "; ".join(g("affiliations", []) or []),
#         "url": g("url"),
#     }
# 
# def map_pmid_to_pmcid_safe(pmid: str) -> Optional[str]:
#     if pmid2pmcid is None: return None
#     try: return pmid2pmcid(pmid)
#     except Exception: return None
# 
# def search_and_collect(query: str, retmax: int, sleep_every: int = 10, sleep_s: float = 0.35) -> List[Dict[str, Any]]:
#     fetch = PubMedFetcher()
#     pmids = fetch.pmids_for_query(query, retmax=retmax)
#     results: List[Dict[str, Any]] = []
#     for i, pmid in enumerate(pmids, 1):
#         if i % sleep_every == 0: sleep(sleep_s)
#         art = fetch_article(fetch, pmid)
#         meta = one_meta(art)
#         meta["pmcid"] = map_pmid_to_pmcid_safe(pmid)
#         results.append(meta)
#     return results
# 
# def save_results(rows: List[Dict[str, Any]], out_prefix: Path):
#     out_prefix.parent.mkdir(parents=True, exist_ok=True)
#     jsonl_path = out_prefix.with_suffix(".jsonl")
#     with open(jsonl_path, "w", encoding="utf-8") as f:
#         for r in rows: f.write(json.dumps(r, ensure_ascii=False) + "\n")
#     csv_path = out_prefix.with_suffix(".csv")
#     pd.DataFrame(rows).to_csv(csv_path, index=False, quoting=csv.QUOTE_MINIMAL)
#     return jsonl_path, csv_path
# 
# def parse_args_notebook_safe(parser: argparse.ArgumentParser):
#     if any(m in sys.modules for m in ("ipykernel", "google.colab")):
#         args, _ = parser.parse_known_args(); return args
#     return parser.parse_args()
# 
# def main():
#     parser = argparse.ArgumentParser(description="Trouver des références PubMed pour des mots-clés donnés.")
#     parser.add_argument("--query", action="append", help="Requête PubMed (peut être répétée).")
#     parser.add_argument("--since", type=int, default=2024, help="Année min (dp).")
#     parser.add_argument("--until", type=int, default=2025, help="Année max (dp).")
#     parser.add_argument("--humans", action="store_true", help="Filtre humans[MeSH Terms]")
#     parser.add_argument("--pmc_oa", action="store_true", help="PMC Open Access uniquement")
#     parser.add_argument("--retmax", type=int, default=300, help="Nb max d'articles par requête")
#     parser.add_argument("--outdir", type=Path, default=Path("results_pubmed"), help="Dossier de sortie")
#     args = parse_args_notebook_safe(parser)
# 
#     if not os.environ.get("NCBI_API_KEY"):
#         print("⚠️  NCBI_API_KEY non défini — optionnel mais recommandé.")
# 
#     queries = args.query if args.query else build_default_queries()
#     timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
#     all_rows: List[Dict[str, Any]] = []
# 
#     for idx, base_q in enumerate(queries, 1):
#         q = apply_filters(base_q, since=args.since, until=args.until, humans=args.humans, pmc_oa=args.pmc_oa)
#         print(f"[{idx}/{len(queries)}] Query:\n{q}\n")
#         rows = search_and_collect(q, retmax=args.retmax)
#         print(f"  -> {len(rows)} résultats")
#         all_rows.extend([dict(r, query=q) for r in rows])
#         prefix = args.outdir / f"q{idx}_{timestamp}"
#         jsonl_path, csv_path = save_results(rows, prefix)
#         print(f"  Saved: {jsonl_path} ; {csv_path}")
# 
#     if all_rows:
#         merged_prefix = args.outdir / f"merged_{timestamp}"
#         jsonl_path, csv_path = save_results(all_rows, merged_prefix)
#         print(f"\nMerged files:\n  {jsonl_path}\n  {csv_path}")
#     else:
#         print("Aucun résultat — assouplis la requête ou augmente --retmax.")
# 
# def run_finder(queries: Optional[List[str]] = None, since: int = 2024, until: int = 2025,
#                humans: bool = True, pmc_oa: bool = False, retmax: int = 300, outdir: str = "results_pubmed"):
#     if not queries: queries = build_default_queries()
#     if not os.environ.get("NCBI_API_KEY"):
#         print("⚠️  NCBI_API_KEY non défini — optionnel mais recommandé.")
#     qlist = [apply_filters(q, since=since, until=until, humans=humans, pmc_oa=pmc_oa) for q in queries]
#     timestamp = datetime.now().strftime("%Y%m%d-%H%M%S"); all_rows: List[Dict[str, Any]] = []
#     for idx, q in enumerate(qlist, 1):
#         print(f"[{idx}/{len(qlist)}] Query:\n{q}\n")
#         rows = search_and_collect(q, retmax=retmax)
#         print(f"  -> {len(rows)} résultats")
#         all_rows.extend([dict(r, query=q) for r in rows])
#         prefix = Path(outdir) / f"q{idx}_{timestamp}"
#         jsonl_path, csv_path = save_results(rows, prefix)
#         print(f"  Saved: {jsonl_path} ; {csv_path}")
#     if all_rows:
#         merged_prefix = Path(outdir) / f"merged_{timestamp}"
#         jsonl_path, csv_path = save_results(all_rows, merged_prefix)
#         print(f"\nMerged files:\n  {jsonl_path}\n  {csv_path}")
#     else:
#         print("Aucun résultat.")
# 
# if __name__ == "__main__":
#     main()
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile finder_pubmed_keywords.py
# # -*- coding: utf-8 -*-
# from __future__ import annotations
# import argparse, csv, json, os, sys
# from pathlib import Path
# from time import sleep
# from datetime import date, datetime
# from typing import List, Dict, Any, Optional
# 
# import pandas as pd
# from tenacity import retry, stop_after_attempt, wait_exponential
# from metapub import PubMedFetcher
# try:
#     from metapub.convert import pmid2pmcid
# except Exception:
#     pmid2pmcid = None
# 
# # ---------- helpers ----------
# def _to_str(x):
#     """Convertit en str tout objet non JSON-sérialisable (dates, etc.)."""
#     if isinstance(x, (datetime, date)):
#         return x.isoformat()
#     return x if isinstance(x, (str, int, float, bool)) or x is None else str(x)
# 
# # ---------- requêtes ----------
# def build_default_queries() -> List[str]:
#     q1 = ('("Parkinson Disease"[MeSH Terms] OR parkinson*[Title/Abstract]) '
#           'AND (technology OR technologies OR wearable* OR sensor* OR '
#           '"digital biomarker*" OR smartphone* OR "mobile app*" OR '
#           '"deep brain stimulation" OR DBS OR "machine learning" OR '
#           '"artificial intelligence" OR AI)')
#     q2 = ('("Alzheimer Disease"[MeSH Terms] OR alzheimer*[Title/Abstract] OR dementia[Title/Abstract]) '
#           'AND (technology OR technologies OR wearable* OR sensor* OR '
#           '"assistive technolog*" OR "smart home" OR "digital biomarker*" OR '
#           '"Internet of Things" OR IoT OR "machine learning" OR '
#           '"artificial intelligence" OR AI OR eye-tracking OR "speech analysis")')
#     return [q1, q2]
# 
# def apply_filters(q: str, since: Optional[int], until: Optional[int], humans: bool, pmc_oa: bool) -> str:
#     parts = [q]
#     if since or until:
#         s = since if since else 1800
#         u = until if until else 3000
#         parts.append(f"({s}:{u}[dp])")
#     if humans:
#         parts.append("humans[MeSH Terms]")
#     if pmc_oa:
#         parts.append("pmc open access[filter]")
#     return " AND ".join(parts)
# 
# # ---------- fetch ----------
# @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=0.8, min=1, max=8))
# def fetch_article(fetcher: PubMedFetcher, pmid: str):
#     return fetcher.article_by_pmid(pmid)
# 
# def one_meta(art) -> Dict[str, Any]:
#     def g(name, default=None):
#         return getattr(art, name, default)
# 
#     # auteurs en texte
#     try:
#         authors = "; ".join(f"{a.lastname} {a.initials}" for a in (art.authors or []))
#     except Exception:
#         authors = None
# 
#     # dates -> texte
#     pubdate = g("pubdate")
#     epubdate = g("epubdate")
#     pubdate = _to_str(pubdate)
#     epubdate = _to_str(epubdate)
# 
#     return {
#         "pmid": g("pmid"),
#         "pmcid": None,  # rempli après
#         "title": g("title"),
#         "journal": g("journal"),
#         "year": g("year"),
#         "volume": g("volume"),
#         "issue": g("issue"),
#         "pages": g("pages"),
#         "doi": g("doi"),
#         "authors": authors,
#         "first_author": g("first_author"),
#         "last_author": g("last_author"),
#         "epubdate": epubdate,
#         "pubdate": pubdate,
#         "mesh": "; ".join(g("meshheadings", []) or []),
#         "affiliations": "; ".join(g("affiliations", []) or []),
#         "url": g("url"),
#     }
# 
# def map_pmid_to_pmcid_safe(pmid: str) -> Optional[str]:
#     if pmid2pmcid is None:
#         return None
#     try:
#         return pmid2pmcid(pmid)
#     except Exception:
#         return None
# 
# def search_and_collect(query: str, retmax: int, sleep_every: int = 10, sleep_s: float = 0.35) -> List[Dict[str, Any]]:
#     fetch = PubMedFetcher()
#     pmids = fetch.pmids_for_query(query, retmax=retmax)
#     results: List[Dict[str, Any]] = []
# 
#     for i, pmid in enumerate(pmids, 1):
#         if i % sleep_every == 0:
#             sleep(sleep_s)  # respect rate limits
#         art = fetch_article(fetch, pmid)
#         meta = one_meta(art)
#         meta["pmcid"] = map_pmid_to_pmcid_safe(pmid)
#         # sécurité: convertir tous les champs non sérialisables -> str
#         meta = {k: _to_str(v) for k, v in meta.items()}
#         results.append(meta)
# 
#     return results
# 
# # ---------- save ----------
# def save_results(rows: List[Dict[str, Any]], out_prefix: Path):
#     out_prefix.parent.mkdir(parents=True, exist_ok=True)
# 
#     # JSONL (sécurisé)
#     jsonl_path = out_prefix.with_suffix(".jsonl")
#     with open(jsonl_path, "w", encoding="utf-8") as f:
#         for r in rows:
#             f.write(json.dumps(r, ensure_ascii=False, default=str) + "\n")
# 
#     # CSV
#     df = pd.DataFrame(rows)
#     csv_path = out_prefix.with_suffix(".csv")
#     df.to_csv(csv_path, index=False, quoting=csv.QUOTE_MINIMAL)
# 
#     return jsonl_path, csv_path
# 
# # ---------- argparse safe ----------
# def parse_args_notebook_safe(parser: argparse.ArgumentParser):
#     if any(m in sys.modules for m in ("ipykernel", "google.colab")):
#         args, _ = parser.parse_known_args()
#         return args
#     return parser.parse_args()
# 
# def main():
#     parser = argparse.ArgumentParser(description="Trouver des références PubMed pour des mots-clés donnés.")
#     parser.add_argument("--query", action="append", help="Requête PubMed (peut être répétée).")
#     parser.add_argument("--since", type=int, default=2024, help="Année min (dp).")
#     parser.add_argument("--until", type=int, default=2025, help="Année max (dp).")
#     parser.add_argument("--humans", action="store_true", help="Filtre humans[MeSH Terms]")
#     parser.add_argument("--pmc_oa", action="store_true", help="PMC Open Access uniquement")
#     parser.add_argument("--retmax", type=int, default=300, help="Nb max d'articles par requête")
#     parser.add_argument("--outdir", type=Path, default=Path("results_pubmed"), help="Dossier de sortie")
#     args = parse_args_notebook_safe(parser)
# 
#     if not os.environ.get("NCBI_API_KEY"):
#         print("⚠️  NCBI_API_KEY non défini — optionnel mais recommandé.")
# 
#     queries = args.query if args.query else build_default_queries()
#     timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
#     all_rows: List[Dict[str, Any]] = []
# 
#     for idx, base_q in enumerate(queries, 1):
#         q = apply_filters(base_q, since=args.since, until=args.until, humans=args.humans, pmc_oa=args.pmc_oa)
#         print(f"[{idx}/{len(queries)}] Query:\n{q}\n")
#         rows = search_and_collect(q, retmax=args.retmax)
#         print(f"  -> {len(rows)} résultats")
#         all_rows.extend([dict(r, query=q) for r in rows])
#         prefix = args.outdir / f"q{idx}_{timestamp}"
#         jsonl_path, csv_path = save_results(rows, prefix)
#         print(f"  Saved: {jsonl_path} ; {csv_path}")
# 
#     if all_rows:
#         merged_prefix = args.outdir / f"merged_{timestamp}"
#         jsonl_path, csv_path = save_results(all_rows, merged_prefix)
#         print(f"\nMerged files:\n  {jsonl_path}\n  {csv_path}")
#     else:
#         print("Aucun résultat — assouplis la requête ou augmente --retmax.")
# 
# # ---------- wrapper notebook ----------
# def run_finder(queries: Optional[List[str]] = None, since: int = 2024, until: int = 2025,
#                humans: bool = True, pmc_oa: bool = False, retmax: int = 300, outdir: str = "results_pubmed"):
#     if not queries:
#         queries = build_default_queries()
#     if not os.environ.get("NCBI_API_KEY"):
#         print("⚠️  NCBI_API_KEY non défini — optionnel mais recommandé.")
#     qlist = [apply_filters(q, since=since, until=until, humans=humans, pmc_oa=pmc_oa) for q in queries]
#     timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
#     all_rows: List[Dict[str, Any]] = []
#     for idx, q in enumerate(qlist, 1):
#         print(f"[{idx}/{len(qlist)}] Query:\n{q}\n")
#         rows = search_and_collect(q, retmax=retmax)
#         print(f"  -> {len(rows)} résultats")
#         all_rows.extend([dict(r, query=q) for r in rows])
#         prefix = Path(outdir) / f"q{idx}_{timestamp}"
#         jsonl_path, csv_path = save_results(rows, prefix)
#         print(f"  Saved: {jsonl_path} ; {csv_path}")
#     if all_rows:
#         merged_prefix = Path(outdir) / f"merged_{timestamp}"
#         jsonl_path, csv_path = save_results(all_rows, merged_prefix)
#         print(f"\nMerged files:\n  {jsonl_path}\n  {csv_path}")
#     else:
#         print("Aucun résultat.")
# 
# if __name__ == "__main__":
#     main()
#

from finder_pubmed_keywords import run_finder
run_finder(since=2014, until=2025, humans=True, retmax=500)