# -*- coding: utf-8 -*-
"""Untitled37.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bsX-vCzvrvW3gnXp6tr2b55ONuKpaFYX
"""

# -*- coding: utf-8 -*-
import csv, codecs, re, json
from collections import Counter, defaultdict

# ======== Mots-clés maladie (redétection PD/AD) ========
PD_TERMS = [
    "parkinson", "parkinson's disease", "freezing of gait", "fog",
    "bradykinesia", "dyskinesia", "tremor", "on-off", "on off",
    "prkn", "levodopa", "l-dopa"
]
AD_TERMS = [
    "alzheimer", "alzheimer's disease", "mci", "mild cognitive impairment",
    "cognitive decline", "dementia", "amyloid", "tau"
]


AREA_KEYWORDS = {
    "Diagnosis": [
        "diagnosis","diagnostic","differential diagnosis","screening","early detection",
        "prodromal","preclinical","case finding","triage","referral","phenotyping",
        "subtype classification","biomarker","fluid biomarker","digital biomarker",
        "clinical biomarker","cut-off","cutoff","threshold","sensitivity","specificity",
        "positive predictive value","negative predictive value","likelihood ratio","roc","auc",
        "calibration","discrimination","confusion matrix","misclassification","risk score",
        "clinical scale","mds-updrs i","updrs i","hoehn–yahr staging","hoehn-yahr staging",
        "moca","mmse","adas-cog","cdr","npi","clock drawing test","verbal fluency",
        "trail making","olfaction test","rem sleep behavior disorder screening",
        "speech screening","gait screening","handwriting analysis","tremor characterization",
        "bradykinesia assessment","rigidity assessment","cognitive screening",
        "neuropsychological testing","gold standard","ground truth","clinician-rated",
        "inter-rater reliability","test–retest","test-retest"
    ],
    "Monitoring Disease Progression": [
        "monitoring","progression","disease course","tracking","longitudinal","follow-up",
        "time series","home monitoring","remote monitoring","ambulatory","real-world data",
        "daily living","ecological validity","symptom diary","motor fluctuations",
        "on–off state","on-off state","dyskinesia burden","tremor severity","gait speed",
        "step length","cadence","stride variability","freezing of gait episodes","falls",
        "balance","postural sway","actigraphy","activity counts","sleep quality",
        "circadian rhythm","fatigue","apathy","depression score","anxiety score",
        "quality of life","mds-updrs ii","mds-updrs iii","updrs motor",
        "hoehn–yahr transition","hoehn-yahr transition","progression rate",
        "responder trajectory","deterioration","stabilization","alerting",
        "deterioration detection","adherence tracking","compliance","telemonitoring",
        "remote assessment","ecological momentary assessment"
    ],
    "Predicting Response to Treatment": [
        "treatment response","therapy response","medication response","drug response",
        "dose response","levodopa challenge","l-dopa response","on–off prediction",
        "on-off prediction","wearing-off","titration","optimization","augmentation",
        "adverse effects","side effects","dyskinesia induction","treatment outcome",
        "outcome prediction","responder classification","non-responder",
        "time-to-response","therapeutic window","pharmacodynamics","pharmacokinetics",
        "adherence","persistence","concordance","dosage adjustment","dose recommendation",
        "therapeutic monitoring","biomarkers of response","eeg response","emg response",
        "gait response","speech response","cognitive response","memantine",
        "cholinesterase inhibitor","donepezil","rivastigmine","galantamine",
        "deep brain stimulation outcome","stimulation parameter tuning",
        "stimulation target selection","benefit–risk","benefit-risk","benefit maintenance",
        "relapse","remission","washout"
    ],
    "Rehabilitation": [
        "rehabilitation","rehab","therapy","physiotherapy","physical therapy",
        "occupational therapy","speech therapy","language therapy","cognitive stimulation",
        "cognitive training","computerized cognitive training","dual-task training",
        "dual task","gait training","balance training","postural control","motor learning",
        "neuroplasticity","cueing","auditory cueing","rhythmic auditory stimulation",
        "visual cueing","haptic cueing","metronome","treadmill training",
        "body-weight support","anti-gravity treadmill","exergames","serious games",
        "telerehabilitation","home-based rehab","vr therapy","ar therapy","mirror therapy",
        "constraint-induced therapy","task-oriented training","adl training","iadl training",
        "fall prevention","freezing of gait management","swallowing therapy","voice training",
        "respiratory training","fine motor training","hand dexterity","upper-limb therapy",
        "lower-limb therapy","adherence to therapy","engagement","usability","satisfaction",
        "fatigue management","personalization","progression of exercises"
    ]
}


ANALYTICAL_KEYWORDS = {
    "Descriptive": [
        "descriptive statistics","summary statistics","central tendency","mean","median","mode",
        "dispersion","variance","standard deviation","interquartile range","histogram",
        "density plot","boxplot","violin plot","correlation","pearson correlation",
        "spearman correlation","association","contingency table","cross-tabulation",
        "prevalence","incidence","frequency","proportion","rate","trend analysis",
        "time-series summary","seasonality check","moving average","smoothing",
        "baseline characterization","cohort profile","subgroup analysis","stratification",
        "effect size","cohen’s d","cohen's d","standardized mean difference",
        "confidence interval","standard error","hypothesis testing","t-test","anova",
        "nonparametric test","mann–whitney","mann-whitney","kruskal–wallis","kruskal-wallis",
        "chi-square","goodness-of-fit","bland–altman","bland-altman","agreement analysis",
        "icc","intraclass correlation","reliability","validity","construct validity",
        "convergent validity","descriptive visualization","feature distribution",
        "missingness pattern","data quality check"
    ],
    "Predictive": [
        "prediction","prognostic modeling","classification","regression",
        "binary classification","multiclass classification","imbalanced learning",
        "cross-validation","train–test split","train-test split","holdout","nested cv",
        "hyperparameter tuning","grid search","random search","bayesian optimization",
        "feature engineering","feature selection","recursive feature elimination",
        "regularization","l1","l2","elastic net","logistic regression","linear regression",
        "random forest","gradient boosting","xgboost","lightgbm","catboost","svm","k-nn",
        "knn","naive bayes","decision tree","ensemble","stacking","bagging","deep learning",
        "neural network","cnn","rnn","lstm","gru","transformer","autoencoder","hmm",
        "markov model","survival analysis","cox model","time-to-event","roc","auc","pr-auc",
        "accuracy","f1-score","f1","precision","recall","sensitivity","specificity",
        "calibration curve","brier score","shap","lime","interpretability"
    ],
    "Prescriptive": [
        "prescriptive analytics","decision support","clinical decision support system","cdss",
        "guideline-based recommendation","rule-based system","knowledge-based system",
        "care pathway optimization","protocol optimization","resource allocation","scheduling",
        "triage","prioritization","alerting thresholds","dose optimization",
        "therapy personalization","recommender system","optimization","linear programming",
        "integer programming","mixed-integer programming","dynamic programming",
        "stochastic programming","robust optimization","multi-objective optimization",
        "pareto front","reinforcement learning","rl","q-learning","deep rl",
        "policy gradient","markov decision process","mdp","partially observable mdp","pomdp",
        "bayesian network","influence diagram","decision tree (prescriptive)","utility function",
        "cost-effectiveness","cost–utility","cost-utility","mcda","ahp","topsis",
        "what-if analysis","scenario analysis","simulation","discrete-event simulation",
        "agent-based simulation","digital twin (decision layer)","workflow integration",
        "human-in-the-loop"
    ]
}

# -------- utils --------
WORD_BOUNDARY = r"(?<![0-9a-z]){term}(?![0-9a-z])"
def norm(s): return (s or "").lower().replace("’","'")

def terms_matched(text, terms):
    """Retourne la liste des termes trouvés (tolère 'on-off' == 'on off')."""
    found = []
    t = norm(text)
    for term in terms:
        patt = term.replace("-", "[ -]")
        pattern = WORD_BOUNDARY.format(term=re.escape(patt))
        if re.search(pattern, t):
            found.append(term)
    return found

def classify_row(title, keywords, abstract):
    blob = " ".join([norm(title), norm(keywords), norm(abstract)])
    # Disease
    diseases = set()
    pd_hits = terms_matched(blob, PD_TERMS)
    ad_hits = terms_matched(blob, AD_TERMS)
    if pd_hits: diseases.add("PD")
    if ad_hits: diseases.add("AD")
    # Areas
    area_labels, area_triggers = [], {}
    for area, terms in AREA_KEYWORDS.items():
        hits = terms_matched(blob, terms)
        if hits:
            area_labels.append(area)
            area_triggers[area] = hits
    # Analytical
    analytical_labels, analytical_triggers = [], {}
    for cat, terms in ANALYTICAL_KEYWORDS.items():
        hits = terms_matched(blob, terms)
        if hits:
            analytical_labels.append(cat)
            analytical_triggers[cat] = hits
    triggers = {"PD": pd_hits, "AD": ad_hits, "Area": area_triggers, "Analytical": analytical_triggers}
    return diseases, area_labels, analytical_labels, triggers

# -------- comptages globaux --------
area_counts        = Counter()
analytical_counts  = Counter()
disease_counts     = Counter()
area_triggers      = defaultdict(Counter)        # area -> term -> count
analytical_triggers= defaultdict(Counter)        # analytical -> term -> count

# -------- entrées/sorties --------
INPUTS = [
    'SDprocessedNew062422.csv',
    'PMCprocessedNew062422.csv',
    'IEEEprocessedNew062422.csv',
    'MDPIprocessedNew062422.csv',
]
out_enriched = 'AllProcessed_with_axes.csv'
out_summary  = 'AxisSummary.csv'

with codecs.open(out_enriched, 'w', encoding='utf8') as fout:
    w = csv.writer(fout, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
    headers = ['Title','Authors','Published in','Year','Document Type','Keywords','Abstract','Identifier','Reason',
               'Disease','AreaLabels','AnalyticalLabels','AxisTriggers']
    w.writerow(headers)

    for path in INPUTS:
        try:
            with codecs.open(path, 'r', encoding='utf8') as fin:
                r = csv.reader(fin, delimiter=',', quotechar='"')
                _ = next(r, None)  # header
                for row in r:
                    # structure attendue (Script 1): colonnes 0..8
                    title     = row[0] if len(row)>0 else ''
                    authors   = row[1] if len(row)>1 else ''
                    journal   = row[2] if len(row)>2 else ''
                    year      = row[3] if len(row)>3 else ''
                    doctype   = row[4] if len(row)>4 else ''
                    keywords  = row[5] if len(row)>5 else ''
                    abstract  = row[6] if len(row)>6 else ''
                    ident     = row[7] if len(row)>7 else ''
                    reason    = row[8] if len(row)>8 else ''

                    diseases, areas, analyticals, trig = classify_row(title, keywords, abstract)

                    # comptages
                    for d in diseases: disease_counts[d] += 1
                    for a in areas:
                        area_counts[a] += 1
                        for t, n in Counter(trig["Area"].get(a, [])).items():
                            area_triggers[a][t] += n
                    for c in analyticals:
                        analytical_counts[c] += 1
                        for t, n in Counter(trig["Analytical"].get(c, [])).items():
                            analytical_triggers[c][t] += n

                    w.writerow([
                        title, authors, journal, year, doctype, keywords, abstract, ident, reason,
                        "|".join(sorted(list(diseases))) if diseases else "",
                        "|".join(areas) if areas else "",
                        "|".join(analyticals) if analyticals else "",
                        json.dumps(trig, ensure_ascii=False)
                    ])
        except FileNotFoundError:
            print(f"[WARN] Fichier introuvable: {path}")

# ----- Résumé des comptages -----
with codecs.open(out_summary, 'w', encoding='utf8') as fsum:
    w = csv.writer(fsum, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
    w.writerow(['Type','Label','Count','TopTerms'])
    # Disease
    for d, n in disease_counts.most_common():
        w.writerow(['Disease', d, n, ''])
    # Areas
    for a, n in area_counts.most_common():
        top_terms = ", ".join([t for t,_ in area_triggers[a].most_common(5)])
        w.writerow(['Area', a, n, top_terms])
    # Analytical
    for c, n in analytical_counts.most_common():
        top_terms = ", ".join([t for t,_ in analytical_triggers[c].most_common(5)])
        w.writerow(['Analytical', c, n, top_terms])

print("OK. Écrits:", out_enriched, "et", out_summary)