# -*- coding: utf-8 -*-
"""Untitled35.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wY6q3bQ3MiyEKdF59GWlzytvCuQarrgy
"""

# -*- coding: utf-8 -*-
import csv
import codecs
import difflib as df

# ===================== Mots-clés MALADIE (PD + AD) =====================
diseaseFilter = [
    # PD
    "parkinson", "parkinson's disease", "freezing of gait", "fog",
    "bradykinesia", "dyskinesia", "tremor", "on-off", "on-off states",
    "prkn", "levodopa", "l-dopa",
    # AD
    "alzheimer", "alzheimer's disease", "mci", "mild cognitive impairment",
    "cognitive decline", "dementia", "amyloid", "tau"
]

# ===================== Techno à INCLURE (large) =====================
technologyFilter = [
    "acceleration","cps","cyber-physique-system",
    "magnetometer","gyroscope","gyro","acc",
    "exoskeleton device","inertial","inertial sensor","inertial measurement unit","imu",
    "video recording","video camera","camera",
    "ehealth","technology","remote monitoring","home monitoring","telemedicine",
    "mobile phone","mobile application","mobile health","precision medicine","digital health",
    "wearable","wearable technology","wearable accelerometer","wearable sensor","biosensor",
    "magneto inertial sensor","wearable inertial sensors","sensor","smartphone",
    "force plate","force sensor","pressure sensor","gait mat",
    "augmented reality","virtual reality","kinect",
    "ecg","eeg","emg","eog","meg","electrocardiography","electroencephalography",
    "electromyography","electrooculography","magnetoencephalography",
    "machine learning","classification","supervised learning","neural networks","deep learning",
    "random forest","svm","xgboost","bayesian network","naive bayes","gait classification",
    "auditory cue","auditory cueing","visual cue","haptic cue","vibration",
    "speech","speech disorder","speech recognition",
    "music therapy","smart application","smartphone application","smart watch","fitness band",
    "smart pen","touchscreen","digitized tablet","leap motion","computer vision",
    "optical motion capturing system","cognitive assessment tools","memory training apps",
    "digital cognitive therapy","virtual reality therapy","augmented reality cognitive exercises",
    "brain-computer interface","digital biomarkers","cognitive decline tracking",
    "eeg for dementia","speech recognition for cognitive impairment",
    "facial recognition for emotion tracking","digital puzzles for alzheimer’s therapy",
    "remote caregiver monitoring","wearable devices for alzheimer’s","dementia monitoring sensors",
    "smart home","activity tracking","neurofeedback systems","cognitive games",
    "smartphone-based cognitive tests","machine learning for dementia prediction",
    "smart pill dispensers","gps tracking for alzheimer’s patients",
    "speech-based cognitive evaluation","automated memory training systems",
    "caregiver support apps","ai-based cognitive stimulation","in-home monitoring systems",
    "digital alzheimer’s disease diaries","daily activity analysis apps",
    "brain health tracking apps","ai-driven dementia diagnosis tools",
    "fall detection devices","interactive therapy devices",
    "sensory cueing devices for memory","cognitive intervention software",
    "gait analysis for dementia patients","patient monitoring systems",
    "sleep tracking for alzheimer’s patients","automated fall alert systems",
    "neuroimaging analysis tools for alzheimer’s","multi-sensory cognitive stimulation",
    "personalized cognitive care technology","smart glasses for alzheimer’s therapy",
    "assistive robots for dementia care","digital assistants for alzheimer’s patients",
    "location tracking devices","alzheimer’s disease care management platforms"
]

# ===================== Animaux à EXCLURE =====================
ignoreList = ["mice","mouse","rat","rats","animal","primates","marmoset","monkey","monkeys"]

# ===================== Techno à EXCLURE (IRM/DBS, etc.) =====================
technologyIgnore = [
    "mri","fmri","magnetic resonance imaging","magnetic resonance images",
    "spect","pet","positron emission tomography","single-photon emission computed tomography",
    "deep brain stimulation","dbs","transcranial","neuroimaging","brain imaging"
]

keywordCount = {}
newResults = []
techIgnores = []
animalIgnores = []
rejected = []

def year_in_range(y):
    """Filtre année 2014–2025 (inclus). Si l'année est absente/invalide -> on ne filtre pas."""
    try:
        yy = int(str(y).strip()[:4])
        return 2014 <= yy <= 2025
    except:
        return True

def scan_any(text, terms):
    """Présence naive d'un terme dans le texte (avec ponctuations usuelles)."""
    t = (text or "").lower()
    for term in terms:
        if f" {term} " in f" {t} " or f"{term}," in t or f"{term}." in t or f"{term};" in t or f"{term}-" in t:
            return True
    return False

def collect_in(text, terms):
    """Retourne la liste des termes trouvés dans le texte."""
    t = (text or "").lower()
    hits = []
    for term in terms:
        if f" {term} " in f" {t} " or f"{term}," in t or f"{term}." in t or f"{term};" in t or f"{term}-" in t:
            hits.append(term)
    return hits

def process_source(in_path, keywords_idx, abstract_idx, year_idx, kw_sep, out_path, encoding='utf8'):
    global newResults, techIgnores, animalIgnores, rejected, keywordCount

    with codecs.open(in_path, 'r', encoding=encoding) as csvfile:
        filereader = csv.reader(csvfile, delimiter=',', quotechar='"')
        _ = next(filereader, None)  # header
        for row in filereader:
            # Filtre année si possible
            if year_idx is not None and len(row) > year_idx and not year_in_range(row[year_idx]):
                rejected.append(row)
                continue

            # Champs de base
            title = row[0] if len(row) > 0 else ""
            abstract = (row[abstract_idx].lower().replace('&rsquo;', "'") if len(row) > abstract_idx else "")
            if keywords_idx is None or len(row) <= keywords_idx or row[keywords_idx] == "":
                rejected.append(row)
                continue
            keywords = [k.strip() for k in row[keywords_idx].split(kw_sep) if k.strip()]

            # 1) Exclure animaux (keywords puis abstract)
            ign = False
            for k in keywords:
                if df.get_close_matches(k.lower(), ignoreList, n=1, cutoff=0.9):
                    row.append('Keywords ' + k)
                    animalIgnores.append(row); ign = True; break
            if ign: continue
            if scan_any(abstract, ignoreList):
                row.append('Abstract animals')
                animalIgnores.append(row); continue

            # 2) Exclure imagerie/DBS (keywords puis abstract)
            for k in keywords:
                if df.get_close_matches(k.lower(), technologyIgnore, n=1, cutoff=0.9):
                    row.append('Keywords ' + k)
                    techIgnores.append(row); ign = True; break
            if ign: continue
            if scan_any(abstract, technologyIgnore):
                row.append('Abstract imaging/DBS')
                techIgnores.append(row); continue

            # 3) Maladie PD/AD (keywords puis abstract)
            disease_ok = False
            for k in keywords:
                if df.get_close_matches(k.lower(), diseaseFilter, n=1, cutoff=0.9):
                    disease_ok = True; break
            if not disease_ok and scan_any(abstract, diseaseFilter):
                disease_ok = True
            if not disease_ok:
                rejected.append(row); continue

            # 4) Techno incluse (keywords puis abstract)
            match = False
            abstractKeys = []

            for t in keywords:
                if df.get_close_matches(t.lower(), technologyFilter, n=1, cutoff=0.9):
                    match = True
                    row.append('Keywords ' + t)
                    break
                else:
                    for w in t.split():
                        if df.get_close_matches(w.lower(), technologyFilter, n=1, cutoff=0.9):
                            row.append('Keywords ' + w)
                            match = True
                            break
                if match: break

            # ajoute techno trouvées dans l'abstract
            hits = collect_in(abstract, technologyFilter)
            if hits:
                if match:
                    s = row[-1] + ' ' + 'Abstract ' + ', '.join(hits)
                    row[-1] = s
                else:
                    row.append('Abstract ' + ', '.join(hits))
                    match = True
                abstractKeys = hits

            if match:
                newResults.append(row)
                # compteur techno indicatif
                totalKeys = list(keywords)
                for m in abstractKeys:
                    if m not in totalKeys:
                        totalKeys.append(m)
                for i in totalKeys:
                    if df.get_close_matches(i.lower(), technologyFilter, n=1, cutoff=0.9):
                        keywordCount[i.lower()] = keywordCount.get(i.lower(), 0) + 1
            else:
                rejected.append(row)

    # écriture du CSV “processed” homogène (9 colonnes)
    if out_path:
        with codecs.open(out_path, 'w', encoding='utf8') as csvfile:
            w = csv.writer(csvfile, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
            w.writerow(['Title','Authors','Published in','Year','Document Type','Keywords','Abstract','Identifier','Reason'])
            for line in newResults:
                pack = line[:8]
                pack.append(line[-1] if len(line) > 8 else '')
                w.writerow(pack)

# ===================== TRAITEMENT DES 4 SOURCES =====================
newResults, techIgnores, animalIgnores, rejected = [], [], [], []

# 1) SD (encodage ISO-8859-1, keywords=col6, abstract=col5, year=col3, sep=",")
process_source('sd_2020_21_english.csv',
               keywords_idx=6, abstract_idx=5, year_idx=3, kw_sep=",",
               out_path='SDprocessedNew062422.csv', encoding='ISO-8859-1')

# 2) PMC (utf8, keywords=col5, abstract=col6, year=col3, sep=",")
process_source('PMC_E_full06222022.csv',
               keywords_idx=5, abstract_idx=6, year_idx=3, kw_sep=",",
               out_path='PMCprocessedNew062422.csv', encoding='utf8')

# 3) IEEE (utf8, keywords=col5 séparés par ";", abstract=col6, year=col3)
process_source('ieee2020_21.csv',
               keywords_idx=5, abstract_idx=6, year_idx=3, kw_sep=";",
               out_path='IEEEprocessedNew062422.csv', encoding='utf8')

# 4) MDPI (utf8, keywords=col5, abstract=col6, year=col3, sep=",")
process_source('mdpi_processed_2020_21.csv',
               keywords_idx=5, abstract_idx=6, year_idx=3, kw_sep=",",
               out_path='MDPIprocessedNew062422.csv', encoding='utf8')

# ===================== TABLEAUX ANNEXES =====================
with codecs.open('TechIgnores062422.csv', 'w', encoding='utf8') as f:
    w = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
    w.writerow(['Title','Authors','Published in','Year','Document Type','Keywords','Abstract','Identifier','Reason'])
    for line in techIgnores:
        pack = line[:8]; pack.append(line[-1] if len(line)>8 else '')
        w.writerow(pack)

with codecs.open('NonHumanIgnores062422.csv', 'w', encoding='utf8') as f:
    w = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
    w.writerow(['Title','Authors','Published in','Year','Document Type','Keywords','Abstract','Identifier','Reason'])
    for line in animalIgnores:
        pack = line[:8]; pack.append(line[-1] if len(line)>8 else '')
        w.writerow(pack)

with codecs.open('Rejected062422.csv', 'w', encoding='utf8') as f:
    w = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
    w.writerow(['Title','Authors','Published in','Year','Document Type','Keywords','Abstract','Identifier'])
    for line in rejected:
        w.writerow(line[:8])

print("Done. Accepted:", len(newResults), "| TechIgnores:", len(techIgnores), "| AnimalIgnores:", len(animalIgnores), "| Rejected:", len(rejected))

